# Home_Sales
Big Data, Module 22

# About
This project is practice working with Big Data. Specifically, Spark SQL is initially used to query questions about home sales data. PySpark is then utilized and the dataset is cached as a table. The most demanding query is re-run on that table. The dataset is then partitioned by a specific data column and that is placed in a parquet temporary table. The biggest query is again re-run and compared. 

# Getting Started / Installation
This project was written in a Google Colab notebook. To re-create this project you will need to find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version.

Currently we are using this one:
spark_version = 'spark-3.4.1'
os.environ['SPARK_VERSION']=spark_version

Next Install Spark and Java
!apt-get update
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz
!tar xf $SPARK_VERSION-bin-hadoop3.tgz
!pip install -q findspark

Finally Set the Environment Variables
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = f"/content/{spark_version}-bin-hadoop3"

The following imports and packages are also necessary:
import findspark
findspark.init()

from pysoark,sql import SparkSession
import time

Create a spark session
spark = SparkSession.builder.appName('SparkSQL').getOrCreate()

Finally, our data was read from an AWS S3 bucket and changed into a DataFrame as follows:
from pyspark import SparkFiles
url = "https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv"
spark.sparkContext.addFile(url)
df = spark.read.csv(SparkFiles.get("home_sales_revised.csv"), sep=",", header=True, ignoreLeadingWhiteSpace=True)
df.show()

# Analysis
General queries about the dataset were made. The run time for querying this dataset was measured for the largest query made. In Spark SQL, start_time = time.time() was used to calcuate the run time as .0968 seconds. 

The table was then cached using spark.sql("cache table home_data") and the same large query was re-run, the run time was .7209 seconds. 

Finally, the table was partitioned on the "date_built" column using: df.write.partitionBy('date_built').mode('overwrite').parquet('home_data_partitioned')
and read in parquet with p_df = spark.read.parquet('home_data_partitioned'). A tempview was created for the parquet data and the largest query re-run, the run time was .6207 seconds. 

# Summary
Despite the dataset in this project being small for "Big Data" standards. The methods of running queries in different formats still proved beneficial for improving run time. The final partitioned and parquet formatted data did prove to have the fastest run time of .6207 seconds. 

# References
Data for this dataset was generated by edX Boot Camps LLC, and is intended for educational purposes only.